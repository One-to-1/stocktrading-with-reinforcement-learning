{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and Dependencies\n",
    "Install and import required libraries including gym, tensorflow/pytorch, pandas-datareader, numpy, and matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "# !pip install gym tensorflow pandas-datareader numpy matplotlib\n",
    "\n",
    "# Import required libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection and Preprocessing\n",
    "Fetch historical stock data, calculate technical indicators, and prepare the data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data shape: (2516, 7)\n",
      "Processed data shape: (2497, 11)\n",
      "\n",
      "Columns: Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits',\n",
      "       'SMA', 'EMA', 'Momentum', 'Volatility'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>SMA</th>\n",
       "      <th>EMA</th>\n",
       "      <th>Momentum</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-02 00:00:00-05:00</th>\n",
       "      <td>5.902122</td>\n",
       "      <td>5.914473</td>\n",
       "      <td>5.825901</td>\n",
       "      <td>5.900615</td>\n",
       "      <td>698342400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.210872</td>\n",
       "      <td>6.161553</td>\n",
       "      <td>-0.362122</td>\n",
       "      <td>0.204166</td>\n",
       "      <td>0.005803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-03 00:00:00-05:00</th>\n",
       "      <td>5.879825</td>\n",
       "      <td>6.031362</td>\n",
       "      <td>5.857230</td>\n",
       "      <td>6.002139</td>\n",
       "      <td>615328000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.188051</td>\n",
       "      <td>6.146371</td>\n",
       "      <td>-0.001806</td>\n",
       "      <td>0.200499</td>\n",
       "      <td>0.017206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-04 00:00:00-05:00</th>\n",
       "      <td>5.926823</td>\n",
       "      <td>5.976231</td>\n",
       "      <td>5.771369</td>\n",
       "      <td>5.785830</td>\n",
       "      <td>757652000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.159552</td>\n",
       "      <td>6.112034</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>0.215356</td>\n",
       "      <td>-0.036039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-05 00:00:00-05:00</th>\n",
       "      <td>5.803303</td>\n",
       "      <td>5.904830</td>\n",
       "      <td>5.749677</td>\n",
       "      <td>5.888561</td>\n",
       "      <td>850306800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.136776</td>\n",
       "      <td>6.090751</td>\n",
       "      <td>0.021991</td>\n",
       "      <td>0.218872</td>\n",
       "      <td>0.017755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-08 00:00:00-05:00</th>\n",
       "      <td>5.895489</td>\n",
       "      <td>5.961467</td>\n",
       "      <td>5.844575</td>\n",
       "      <td>5.848190</td>\n",
       "      <td>478270800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.109872</td>\n",
       "      <td>6.067650</td>\n",
       "      <td>-0.052424</td>\n",
       "      <td>0.219660</td>\n",
       "      <td>-0.006856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Open      High       Low     Close     Volume  \\\n",
       "Date                                                                           \n",
       "2010-02-02 00:00:00-05:00  5.902122  5.914473  5.825901  5.900615  698342400   \n",
       "2010-02-03 00:00:00-05:00  5.879825  6.031362  5.857230  6.002139  615328000   \n",
       "2010-02-04 00:00:00-05:00  5.926823  5.976231  5.771369  5.785830  757652000   \n",
       "2010-02-05 00:00:00-05:00  5.803303  5.904830  5.749677  5.888561  850306800   \n",
       "2010-02-08 00:00:00-05:00  5.895489  5.961467  5.844575  5.848190  478270800   \n",
       "\n",
       "                           Dividends  Stock Splits       SMA       EMA  \\\n",
       "Date                                                                     \n",
       "2010-02-02 00:00:00-05:00        0.0           0.0  6.210872  6.161553   \n",
       "2010-02-03 00:00:00-05:00        0.0           0.0  6.188051  6.146371   \n",
       "2010-02-04 00:00:00-05:00        0.0           0.0  6.159552  6.112034   \n",
       "2010-02-05 00:00:00-05:00        0.0           0.0  6.136776  6.090751   \n",
       "2010-02-08 00:00:00-05:00        0.0           0.0  6.109872  6.067650   \n",
       "\n",
       "                           Momentum  Volatility    Return  \n",
       "Date                                                       \n",
       "2010-02-02 00:00:00-05:00 -0.362122    0.204166  0.005803  \n",
       "2010-02-03 00:00:00-05:00 -0.001806    0.200499  0.017206  \n",
       "2010-02-04 00:00:00-05:00 -0.000300    0.215356 -0.036039  \n",
       "2010-02-05 00:00:00-05:00  0.021991    0.218872  0.017755  \n",
       "2010-02-08 00:00:00-05:00 -0.052424    0.219660 -0.006856  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Collection and Preprocessing\n",
    "\n",
    "# Fetch historical stock data\n",
    "def fetch_stock_data(ticker, start_date, end_date):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    df = stock.history(start=start_date, end=end_date)\n",
    "    return df\n",
    "\n",
    "# Calculate technical indicators\n",
    "def calculate_technical_indicators(df):\n",
    "    try:\n",
    "        df['SMA'] = df['Close'].rolling(window=20).mean()\n",
    "        df['EMA'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
    "        df['Momentum'] = df['Close'] - df['Close'].shift(4)\n",
    "        df['Volatility'] = df['Close'].rolling(window=20).std()\n",
    "        df.dropna(inplace=True)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating indicators: {e}\")\n",
    "        return None\n",
    "\n",
    "# Prepare the data for training\n",
    "def prepare_data(df):\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "    df.dropna(inplace=True)\n",
    "    features = df[['SMA', 'EMA', 'Momentum', 'Volatility']].values\n",
    "    labels = df['Return'].values\n",
    "    return features, labels\n",
    "\n",
    "# Example usage\n",
    "ticker = 'AAPL'\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2020-01-01'\n",
    "\n",
    "# Fetch and preprocess data\n",
    "stock_data = fetch_stock_data(ticker, start_date, end_date)\n",
    "print(\"Raw data shape:\", stock_data.shape)\n",
    "stock_data = calculate_technical_indicators(stock_data)\n",
    "print(\"Processed data shape:\", stock_data.shape)\n",
    "print(\"\\nColumns:\", stock_data.columns)\n",
    "features, labels = prepare_data(stock_data)\n",
    "\n",
    "# Display the first few rows of the preprocessed data\n",
    "stock_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Trading Environment\n",
    "Create a custom OpenAI Gym environment that simulates the stock trading environment with state space, action space, and reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6.21087234  6.1615531  -0.36212158  0.20416609]\n"
     ]
    }
   ],
   "source": [
    "# Define Trading Environment\n",
    "from gym import spaces\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0, 0, 0, 0]), \n",
    "            high=np.array([np.inf, np.inf, np.inf, np.inf]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.balance = 10000  # Initial balance\n",
    "        self.shares_held = 0\n",
    "        self.total_shares_sold = 0\n",
    "        self.total_sales_value = 0\n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        obs = self.df.iloc[self.current_step][['SMA', 'EMA', 'Momentum', 'Volatility']].values\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        self._take_action(action)\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        reward = self.balance + (self.shares_held * self.df.iloc[self.current_step]['Close']) - 10000\n",
    "        obs = self._next_observation()\n",
    "        \n",
    "        return obs, reward, done, {}\n",
    "    \n",
    "    def _take_action(self, action):\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            shares_bought = self.balance // current_price\n",
    "            self.balance -= shares_bought * current_price\n",
    "            self.shares_held += shares_bought\n",
    "        \n",
    "        elif action == 2:  # Sell\n",
    "            self.balance += self.shares_held * current_price\n",
    "            self.total_shares_sold += self.shares_held\n",
    "            self.total_sales_value += self.shares_held * current_price\n",
    "            self.shares_held = 0\n",
    "\n",
    "# Example usage\n",
    "env = StockTradingEnv(stock_data)\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Trading Agent\n",
    "Implement a trading agent class with methods for selecting actions, storing experiences, and updating the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yatharth Jain\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self._build_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(24, input_shape=(self.state_size,), activation='relu'),\n",
    "            tf.keras.layers.Dense(24, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n",
    "        return model\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        state = np.reshape(state, [1, self.state_size])\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "            \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([i[0][0] for i in minibatch])\n",
    "        actions = np.array([i[1] for i in minibatch])\n",
    "        rewards = np.array([i[2] for i in minibatch])\n",
    "        next_states = np.array([i[3][0] for i in minibatch])\n",
    "        dones = np.array([i[4] for i in minibatch])\n",
    "\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict(next_states, verbose=0), axis=1)) * (1 - dones)\n",
    "        target_f = self.model.predict(states, verbose=0)\n",
    "        \n",
    "        for i, action in enumerate(actions):\n",
    "            target_f[i][action] = targets[i]\n",
    "        \n",
    "        self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "env = StockTradingEnv(stock_data)\n",
    "state_size = 4  # Number of features\n",
    "action_size = 3 # Hold, Buy, Sell\n",
    "agent = DQNAgent(state_size=state_size, action_size=action_size)\n",
    "\n",
    "# Usage example\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "action = agent.act(state)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "next_state = np.reshape(next_state, [1, state_size])\n",
    "agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "if len(agent.memory) > 32:\n",
    "    agent.replay(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement DQN Architecture\n",
    "Build the Deep Q-Network model architecture with experience replay and target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yatharth Jain\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\layers\\core\\dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument(s) not recognized: {'lr': 0.001}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m state_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     61\u001b[0m action_size \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[1;32m---> 62\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Example of storing an experience\u001b[39;00m\n\u001b[0;32m     65\u001b[0m state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(obs, [\u001b[38;5;241m1\u001b[39m, state_size])\n",
      "Cell \u001b[1;32mIn[5], line 15\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[1;34m(self, state_size, action_size)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.995\u001b[39m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_model()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_model()\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mDQNAgent._build_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m24\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     24\u001b[0m model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_size, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\adam.py:60\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, learning_rate, beta_1, beta_2, epsilon, amsgrad, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, name, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     45\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     59\u001b[0m ):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     61\u001b[0m         learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[0;32m     62\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m     63\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[0;32m     64\u001b[0m         clipnorm\u001b[38;5;241m=\u001b[39mclipnorm,\n\u001b[0;32m     65\u001b[0m         clipvalue\u001b[38;5;241m=\u001b[39mclipvalue,\n\u001b[0;32m     66\u001b[0m         global_clipnorm\u001b[38;5;241m=\u001b[39mglobal_clipnorm,\n\u001b[0;32m     67\u001b[0m         use_ema\u001b[38;5;241m=\u001b[39muse_ema,\n\u001b[0;32m     68\u001b[0m         ema_momentum\u001b[38;5;241m=\u001b[39mema_momentum,\n\u001b[0;32m     69\u001b[0m         ema_overwrite_frequency\u001b[38;5;241m=\u001b[39mema_overwrite_frequency,\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     71\u001b[0m     )\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_1 \u001b[38;5;241m=\u001b[39m beta_1\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_2 \u001b[38;5;241m=\u001b[39m beta_2\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\backend\\tensorflow\\optimizer.py:20\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:38\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[1;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[0;32m     34\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `decay` is no longer supported and will be ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument(s) not recognized: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     41\u001b[0m     name \u001b[38;5;241m=\u001b[39m auto_name(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Argument(s) not recognized: {'lr': 0.001}"
     ]
    }
   ],
   "source": [
    "# Implement DQN Architecture\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def _build_model(self):\n",
    "        # Neural Network for Deep Q-Learning Model\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(24, activation='relu'))\n",
    "        model.add(tf.keras.layers.Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # Copy weights from model to target_model\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.target_model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "    \n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "# Example usage\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Example of storing an experience\n",
    "state = np.reshape(obs, [1, state_size])\n",
    "action = agent.act(state)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "next_state = np.reshape(next_state, [1, state_size])\n",
    "agent.remember(state, action, reward, next_state, done)\n",
    "\n",
    "# Example of updating the policy\n",
    "agent.replay(32)\n",
    "\n",
    "# Update target model\n",
    "agent.update_target_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop Implementation\n",
    "Create the main training loop with epsilon-greedy exploration and model updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop Implementation\n",
    "\n",
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    \n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            agent.update_target_model()\n",
    "            print(f\"episode: {e}/{num_episodes}, score: {time}, e: {agent.epsilon:.2}\")\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backtesting and Evaluation\n",
    "Test the trained agent on historical data and calculate performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting and Evaluation\n",
    "\n",
    "# Test the trained agent on historical data and calculate performance metrics\n",
    "\n",
    "# Define a function to backtest the agent\n",
    "def backtest_agent(env, agent, episodes=10):\n",
    "    total_rewards = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = np.reshape(next_state, [1, state_size])\n",
    "        total_rewards.append(total_reward)\n",
    "    return total_rewards\n",
    "\n",
    "# Calculate performance metrics\n",
    "def calculate_performance_metrics(rewards):\n",
    "    avg_reward = np.mean(rewards)\n",
    "    std_reward = np.std(rewards)\n",
    "    max_reward = np.max(rewards)\n",
    "    min_reward = np.min(rewards)\n",
    "    return avg_reward, std_reward, max_reward, min_reward\n",
    "\n",
    "# Backtest the agent\n",
    "rewards = backtest_agent(env, agent, episodes=10)\n",
    "\n",
    "# Calculate performance metrics\n",
    "avg_reward, std_reward, max_reward, min_reward = calculate_performance_metrics(rewards)\n",
    "\n",
    "# Print performance metrics\n",
    "print(f\"Average Reward: {avg_reward}\")\n",
    "print(f\"Standard Deviation of Reward: {std_reward}\")\n",
    "print(f\"Maximum Reward: {max_reward}\")\n",
    "print(f\"Minimum Reward: {min_reward}\")\n",
    "\n",
    "# Plot the rewards\n",
    "plt.plot(rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Rewards per Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Strategy Visualization\n",
    "Create visualizations of trading actions, portfolio value, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trading Strategy Visualization\n",
    "\n",
    "# Plot trading actions\n",
    "def plot_trading_actions(df, actions):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(df['Close'], label='Close Price')\n",
    "    buy_signals = df[actions == 1]\n",
    "    sell_signals = df[actions == 2]\n",
    "    plt.scatter(buy_signals.index, buy_signals['Close'], marker='^', color='g', label='Buy Signal', alpha=1)\n",
    "    plt.scatter(sell_signals.index, sell_signals['Close'], marker='v', color='r', label='Sell Signal', alpha=1)\n",
    "    plt.title('Trading Strategy Visualization')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Close Price')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Plot portfolio value\n",
    "def plot_portfolio_value(portfolio_values):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(portfolio_values, label='Portfolio Value')\n",
    "    plt.title('Portfolio Value Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('Portfolio Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming `actions` is a list of actions taken by the agent and `portfolio_values` is a list of portfolio values over time\n",
    "actions = [env.action_space.sample() for _ in range(len(stock_data))]  # Random actions for example\n",
    "portfolio_values = [10000 + i * 10 for i in range(len(stock_data))]  # Example portfolio values\n",
    "\n",
    "plot_trading_actions(stock_data, np.array(actions))\n",
    "plot_portfolio_value(portfolio_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
